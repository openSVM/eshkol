;;
;; Vector Calculus and Automatic Differentiation Example
;;

;; Define a 3D vector
(define v1 (vector 1.0 2.0 3.0))
(define v2 (vector 4.0 5.0 6.0))

;; Basic vector operations
(define v-sum (v+ v1 v2))        ;; Vector addition: [5.0, 7.0, 9.0]
(define v-diff (v- v1 v2))       ;; Vector subtraction: [-3.0, -3.0, -3.0]
(define v-prod (v* v1 v2))       ;; Element-wise multiplication: [4.0, 10.0, 18.0]

;; Vector calculus operations
(define dot-prod (dot v1 v2))    ;; Dot product: 32.0
(define cross-prod (cross v1 v2)) ;; Cross product: [-3.0, 6.0, -3.0]
(define v1-norm (norm v1))       ;; Vector norm: 3.74

;; Define a scalar field f(x,y,z) = x^2 + y^2 + z^2
(define (f v)
  (let ((x (vector-ref v 0))
        (y (vector-ref v 1))
        (z (vector-ref v 2)))
    (+ (* x x) (* y y) (* z z))))

;; Define a vector field F(x,y,z) = [x^2, y^2, z^2]
(define (F v)
  (let ((x (vector-ref v 0))
        (y (vector-ref v 1))
        (z (vector-ref v 2)))
    (vector (* x x) (* y y) (* z z))))

;; Compute gradient of scalar field at point v1
;; Result: [2.0, 4.0, 6.0]
(define grad-f (gradient f v1))

;; Compute divergence of vector field at point v1
;; Result: 2.0 + 2.0 + 2.0 = 6.0
(define div-F (divergence F v1))

;; Compute curl of vector field at point v1
;; Result: [0.0, 0.0, 0.0] (since F is a gradient field)
(define curl-F (curl F v1))

;; Compute Laplacian of scalar field at point v1
;; Result: 6.0 (constant second derivatives)
(define laplacian-f (laplacian f v1))

;;
;; Automatic Differentiation Examples
;;

;; Define a function for automatic differentiation
(define (g x)
  (* x x x))  ;; g(x) = x^3

;; Compute the derivative of g at x=2.0
;; Result: 12.0 (since g'(x) = 3x^2)
(define dg/dx (derivative g 2.0))

;; Define a multivariate function
(define (h v)
  (let ((x (vector-ref v 0))
        (y (vector-ref v 1)))
    (+ (* x x y) (* y y))))  ;; h(x,y) = x^2*y + y^2

;; Compute the gradient of h at point [1.0, 2.0]
;; Result: [4.0, 5.0] (since ∂h/∂x = 2xy, ∂h/∂y = x^2 + 2y)
(define grad-h (gradient h (vector 1.0 2.0)))

;; Compute the Jacobian matrix of a vector-valued function
(define (vector-func v)
  (let ((x (vector-ref v 0))
        (y (vector-ref v 1)))
    (vector (* x y) (* x x y))))

;; Result is a 2x2 matrix of partial derivatives
(define jacobian-matrix (jacobian vector-func (vector 1.0 2.0)))

;;
;; Neural Network Example with Automatic Differentiation
;;

;; Define a simple neural network with one hidden layer
(define (neural-net x w1 b1 w2 b2)
  (let* ((h1 (tanh (+ (* w1 x) b1)))  ;; Hidden layer with tanh activation
         (y  (+ (* w2 h1) b2)))       ;; Output layer (linear)
    y))

;; Define a loss function (mean squared error)
(define (mse-loss y-pred y-true)
  (let ((diff (- y-pred y-true)))
    (* diff diff)))

;; Compute gradients with respect to all parameters
(define input 1.0)
(define target 2.0)
(define w1 0.5)
(define b1 0.1)
(define w2 0.3)
(define b2 0.2)

;; Forward pass
(define prediction (neural-net input w1 b1 w2 b2))
(define loss (mse-loss prediction target))

;; Backward pass - compute gradients automatically
(define gradients (gradients loss '(w1 b1 w2 b2)))

;; Extract individual gradients
(define grad-w1 (vector-ref gradients 0))
(define grad-b1 (vector-ref gradients 1))
(define grad-w2 (vector-ref gradients 2))
(define grad-b2 (vector-ref gradients 3))

;; Update parameters using gradient descent
(define learning-rate 0.1)
(define w1-new (- w1 (* learning-rate grad-w1)))
(define b1-new (- b1 (* learning-rate grad-b1)))
(define w2-new (- w2 (* learning-rate grad-w2)))
(define b2-new (- b2 (* learning-rate grad-b2)))

;; Main function to demonstrate the example
(define (main)
  (printf "Vector v1: %v\n" v1)
  (printf "Vector v2: %v\n" v2)
  (printf "v1 + v2: %v\n" v-sum)
  (printf "v1 · v2: %f\n" dot-prod)
  (printf "v1 × v2: %v\n" cross-prod)
  
  (printf "\nGradient of f at v1: %v\n" grad-f)
  (printf "Divergence of F at v1: %f\n" div-F)
  (printf "Curl of F at v1: %v\n" curl-F)
  
  (printf "\nDerivative of g(x) at x=2: %f\n" dg/dx)
  (printf "Gradient of h at [1,2]: %v\n" grad-h)
  
  (printf "\nNeural Network Example:\n")
  (printf "Prediction: %f, Target: %f\n" prediction target)
  (printf "Loss: %f\n" loss)
  (printf "Gradients: w1=%f, b1=%f, w2=%f, b2=%f\n" 
          grad-w1 grad-b1 grad-w2 grad-b2)
  (printf "Updated parameters: w1=%f, b1=%f, w2=%f, b2=%f\n"
          w1-new b1-new w2-new b2-new)
  
  0)
